{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a33b62569854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-010ea6bb0b58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m199\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 199) \n",
    "        #self.fc2 = nn.Linear(512, 512) \n",
    "\n",
    "        self.fc3 = nn.Linear(199, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = (self.fc3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net(num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean0 = (1, 1)\n",
    "cov0 = [[1, 0], [0, 1]]\n",
    "\n",
    "mean1 = (-1, -1)\n",
    "cov1 = [[1, 0], [0, 1]]\n",
    "\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, 1000)\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, 1000)\n",
    "\n",
    "dataset = [(torch.Tensor(point), torch.LongTensor([0])) for point in data0]\n",
    "dataset += [(torch.Tensor(point), torch.LongTensor([1])) for point in data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniform noise\n",
    "\n",
    "mean0 = (1, 1)\n",
    "sigma = 0.5\n",
    "rho = sigma * sigma\n",
    "\n",
    "cov0 = [[rho, 0], [0, rho]]\n",
    "\n",
    "mean1 = (-1, -1)\n",
    "cov1 = [[rho, 0], [0, rho]]\n",
    "\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, 1000)\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, 1000)\n",
    "\n",
    "data = np.concatenate((data0, data1))\n",
    "#data1[:,1] = data1[:,1] - 2\n",
    "#print(data0)\n",
    "\n",
    "input = torch.tensor(data).float()\n",
    "\n",
    "\n",
    "label = [torch.LongTensor(np.random.binomial(1, 0.3, 1)) for point in data0]\n",
    "label += [torch.LongTensor(np.random.binomial(1, 0.7, 1)) for point in data1]\n",
    "\n",
    "label = torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniform noise\n",
    "\n",
    "mean0 = (1, 1)\n",
    "sigma = 0.5\n",
    "rho = sigma * sigma\n",
    "\n",
    "cov0 = [[rho, 0], [0, rho]]\n",
    "\n",
    "mean1 = (-1, -1)\n",
    "cov1 = [[rho, 0], [0, rho]]\n",
    "\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, 1000)\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, 1000)\n",
    "\n",
    "data = np.concatenate((data0, data1))\n",
    "#data1[:,1] = data1[:,1] - 2\n",
    "#print(data0)\n",
    "\n",
    "input = torch.tensor(data).float()\n",
    "\n",
    "\n",
    "label = [torch.LongTensor(np.random.binomial(1, 0.3, 1)) for point in data0]\n",
    "label += [torch.LongTensor(np.random.binomial(1, 0.7, 1)) for point in data1]\n",
    "\n",
    "label = torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "import math\n",
    "\n",
    "class Adam(Optimizer):\n",
    "\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])\n",
    "\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                else:\n",
    "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "\n",
    "                step_size = group['lr'] / bias_correction1\n",
    "\n",
    "                p.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "\n",
    "                update = (exp_avg / denom).view(-1)\n",
    "                if update.shape[0] > 200:\n",
    "\n",
    "                #print(update)\n",
    "\n",
    "                  plt.hist(update.cpu())\n",
    "                  plt.show()\n",
    "                  plt.close()\n",
    "                ###### Zhiyi THIS 'update' is the random variable we care about!\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "mean0 = (1, 1)\n",
    "#sigma = 0.5\n",
    "rho = sigma * sigma\n",
    "\n",
    "cov0 = [[rho, 0], [0, rho]]\n",
    "\n",
    "mean1 = (-1, -1)\n",
    "cov1 = [[rho, 0], [0, rho]]\n",
    "\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, 100)\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, 100)\n",
    "\n",
    "data = np.concatenate((data0, data1))\n",
    "#data1[:,1] = data1[:,1] - 2\n",
    "#print(data0)\n",
    "\n",
    "input = torch.tensor(data).float().cuda()\n",
    "\n",
    "\n",
    "label = [torch.LongTensor(np.random.binomial(1, 0.3, 1)) for point in data0]\n",
    "label += [torch.LongTensor(np.random.binomial(1, 0.7, 1)) for point in data1]\n",
    "\n",
    "label = torch.tensor(label).cuda()\n",
    "\n",
    "model1 = Net(num_classes = 2).cuda()\n",
    "optimizer = Adam(model1.parameters(), lr=1e-3)\n",
    "\n",
    "model1.train()\n",
    "\n",
    "#dataset_size = len(label)\n",
    "#batchsize = 2\n",
    "\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "  print('step: ', i)\n",
    "  optimizer.zero_grad()\n",
    "  #batch_ind = np.random.randint(0, dataset_size, batchsize)\n",
    "\n",
    "  #output = model1(input[batch_ind])\n",
    "  output = model1(input)\n",
    "  output = F.log_softmax(output, dim=0)\n",
    "\n",
    "  #loss = F.nll_loss(output, label[batch_ind])\n",
    "  loss = F.nll_loss(output, label)\n",
    "  \n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if i % 100 == 0:\n",
    "    #with torch.no_grad:\n",
    "      #print(loss)\n",
    "      #output = model1(input)\n",
    "      #output = F.log_softmax(output, dim=0)\n",
    "      #loss = F.nll_loss(output, label)\n",
    "      \n",
    "      #print(avgpred)\n",
    "      pred = output.max(dim=1)[1]\n",
    "      #print(pred == 0)\n",
    "      avgpred1 = F.softmax(output, dim=1)[pred == 0].mean(dim=0)[0].item()\n",
    "      avgpred2 = F.softmax(output, dim=1)[pred == 1].mean(dim=0)[1].item()\n",
    "      avgpred = (avgpred1 + avgpred2)/2\n",
    "\n",
    "      \n",
    "      print(loss.item(), (pred == label).sum().float().item() / len(label), avgpred)\n",
    "#correct[sigma] = (((pred == label).sum().float() / len(label)).item())\n",
    "#losses[sigma] = loss.item()\n",
    "#avg_pred[sigma] = avgpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
